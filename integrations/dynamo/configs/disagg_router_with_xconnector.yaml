Common:
  model: /data/model/DeepSeek-R1-Distill-Llama-8B  # 🔧 使用小模型
  block-size: 32                                    # 🔧 减少块大小
  max-model-len: 8192                              # 🔧 减少最大长度
  router: kv
  kv-transfer-config: '{"kv_connector":"DynamoNixlConnector"}'

Frontend:
  served_model_name: DeepSeek-R1-Distill-Llama-8B
  endpoint: dynamo.Processor.chat/completions
  port: 8000

Processor:
  common-configs: [model, block-size, max-model-len, router]

Router:
  min-workers: 1
  common-configs: [model, block-size, router]

VllmWorker:
  max-num-batched-tokens: 8192                     # 🔧 减少批处理大小
  remote-prefill: true
  conditional-disagg: true
  max-local-prefill-length: 8                      # 🔧 减少本地预填充长度
  max-prefill-queue-size: 2
  tensor-parallel-size: 8
  enable-prefix-caching: true
  gpu-memory-utilization: 0.80                     # 🔧 分离模式使用更保守的内存
  ServiceArgs:
    workers: 1
    resources:
      gpu: '8'
  common-configs: [model, block-size, max-model-len, router, kv-transfer-config]

PrefillWorker:
  max-num-batched-tokens: 8192                     # 🔧 减少批处理大小
  gpu-memory-utilization: 0.80                     # 🔧 添加内存利用率控制
  ServiceArgs:
    workers: 1
    resources:
      gpu: '8'
  common-configs: [model, block-size, max-model-len, kv-transfer-config]

Planner:
  environment: local
  no-operation: true
Common:
  model: /data/model/DeepSeek-R1-Distill-Llama-8B  # 🔧 使用小模型
  block-size: 32                                    # 🔧 减少块大小
  max-model-len: 8192                              # 🔧 减少最大长度
  router: kv
  kv-transfer-config: '{"kv_connector":"DynamoNixlConnector"}'

Frontend:
  served_model_name: DeepSeek-R1-Distill-Llama-8B
  endpoint: dynamo.Processor.chat/completions
  port: 8000

Processor:
  common-configs: [model, block-size, max-model-len, router]

Router:
  min-workers: 1
  common-configs: [model, block-size, router]

VllmWorker:
  max-num-batched-tokens: 8192                     # 🔧 减少批处理大小
  remote-prefill: true
  conditional-disagg: true
  max-local-prefill-length: 8                      # 🔧 减少本地预填充长度
  max-prefill-queue-size: 2
  tensor-parallel-size: 8
  enable-prefix-caching: true
  gpu-memory-utilization: 0.80                     # 🔧 分离模式使用更保守的内存
  ServiceArgs:
    workers: 1
    resources:
      gpu: '8'
  common-configs: [model, block-size, max-model-len, router, kv-transfer-config]

PrefillWorker:
  max-num-batched-tokens: 8192                     # 🔧 减少批处理大小
  gpu-memory-utilization: 0.80                     # 🔧 添加内存利用率控制
  ServiceArgs:
    workers: 1
    resources:
      gpu: '8'
  common-configs: [model, block-size, max-model-len, kv-transfer-config]

Planner:
  environment: local
  no-operation: true
